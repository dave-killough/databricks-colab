{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/Y82oHq9CjWP+g60eEp0W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dave-killough/databricks-colab/blob/main/Databricks_Parallel_Charting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Databricks Pipeline in Colab\n",
        "This notebook is a local development environment for code that will also run in Databricks.  The table of contents on the left can be used as an index for each step in the pipeline.  \n"
      ],
      "metadata": {
        "id": "hglIckLxMFIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Nq9ZmPhqmSqa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9d0zNVuXBZr"
      },
      "outputs": [],
      "source": [
        "%pip install pyspark==3.5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EO 990 Ingest CSV"
      ],
      "metadata": {
        "id": "3f_yAHngLy77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EO 990 Ingest CSV\n",
        "import requests\n",
        "import logging\n",
        "import os\n",
        "\n",
        "if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
        "    dbfs = \"/dbfs\"\n",
        "    folder_out = \"/mnt/eo990pipeline\"\n",
        "else: # local setup - no cluster charges!!\n",
        "    dbfs = \"\"\n",
        "    folder_out = \".\"\n",
        "\n",
        "logging.basicConfig(level=logging.INFO) # Initialize logging\n",
        "logger = logging.getLogger(\"EO-990-Ingest-Master\")\n",
        "\n",
        "def ingest(url, filename=None):\n",
        "    if filename is None:\n",
        "        filename = url.split(\"/\")[-1]  # Extract filename from URL\n",
        "    response = requests.get(url) # Download CSV file\n",
        "    if response.status_code == 200:\n",
        "        with open(f\"{dbfs}{folder_out}/{filename}\", \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "        logger.info(f\"Successfully downloaded {url}\")\n",
        "    else:\n",
        "        logger.error(f\"Failed to download {url}\")\n",
        "\n",
        "eo_urls = [\n",
        "    \"https://www.irs.gov/pub/irs-soi/eo1.csv\",\n",
        "    \"https://www.irs.gov/pub/irs-soi/eo2.csv\",\n",
        "    \"https://www.irs.gov/pub/irs-soi/eo3.csv\",\n",
        "    \"https://www.irs.gov/pub/irs-soi/eo4.csv\"\n",
        "]\n",
        "for url in eo_urls:\n",
        "    ingest(url)\n",
        "bucket = \"https://storage.googleapis.com/benevolentmachines\"\n",
        "ingest(f\"{bucket}/e990_extract.csv\", \"eo990extract.csv\")\n",
        "ingest(f\"{bucket}/gcst.csv\")\n",
        "# end"
      ],
      "metadata": {
        "id": "WDJ8KMxgLaFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EO 990 Prepare E990"
      ],
      "metadata": {
        "id": "FT_hl3AGQNz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EO 990 Prepare E990\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "appName = \"eo990-prepare-e990\"\n",
        "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
        "if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
        "    folder_in = \"/mnt/eo990pipeline\"\n",
        "else: # local spark - no cluster charges!!\n",
        "    folder_in = \".\"\n",
        "\n",
        "from pyspark.sql.types import \\\n",
        "    StructType, StructField, StringType, IntegerType, LongType\n",
        "schema = StructType([\n",
        "    StructField(\"BLOBI\", StringType(), True),\n",
        "    StructField(\"EIN\", StringType(), True),\n",
        "    StructField(\"ReturnTypeCd\", StringType(), True),\n",
        "    StructField(\"TaxPeriodEndDt\", StringType(), True),\n",
        "    StructField(\"BusinessName\", StringType(), True),\n",
        "    StructField(\"BusinessStreet\", StringType(), True),\n",
        "    StructField(\"CityNm\", StringType(), True),\n",
        "    StructField(\"StateAbbreviationCd\", StringType(), True),\n",
        "    StructField(\"ZIPCd\", StringType(), True),\n",
        "    StructField(\"WebsiteAddressTxt\", StringType(), True),\n",
        "    StructField(\"TotalEmployeeCnt\", IntegerType(), True),\n",
        "    StructField(\"TotalVolunteersCnt\", IntegerType(), True),\n",
        "    StructField(\"GrossReceiptsAmt\", LongType(), True),\n",
        "    StructField(\"PYContributionsGrantsAmt\", LongType(), True),\n",
        "    StructField(\"CYContributionsGrantsAmt\", LongType(), True),\n",
        "    StructField(\"PYProgramServiceRevenueAmt\", LongType(), True),\n",
        "    StructField(\"CYProgramServiceRevenueAmt\", LongType(), True),\n",
        "    StructField(\"PYInvestmentIncomeAmt\", LongType(), True),\n",
        "    StructField(\"CYInvestmentIncomeAmt\", LongType(), True),\n",
        "    StructField(\"PYOtherRevenueAmt\", LongType(), True),\n",
        "    StructField(\"CYOtherRevenueAmt\", LongType(), True),\n",
        "    StructField(\"PYTotalRevenueAmt\", LongType(), True),\n",
        "    StructField(\"CYTotalRevenueAmt\", LongType(), True),\n",
        "    StructField(\"PYGrantsAndSimilarPaidAmt\", LongType(), True),\n",
        "    StructField(\"CYGrantsAndSimilarPaidAmt\", LongType(), True),\n",
        "    StructField(\"PYBenefitsPaidToMembersAmt\", LongType(), True),\n",
        "    StructField(\"CYBenefitsPaidToMembersAmt\", LongType(), True),\n",
        "    StructField(\"PYSalariesCompEmpBnftPaidAmt\", LongType(), True),\n",
        "    StructField(\"CYSalariesCompEmpBnftPaidAmt\", LongType(), True),\n",
        "    StructField(\"PYTotalProfFndrsngExpnsAmt\", LongType(), True),\n",
        "    StructField(\"CYTotalProfFndrsngExpnsAmt\", LongType(), True),\n",
        "    StructField(\"CYTotalFundraisingExpenseAmt\", LongType(), True),\n",
        "    StructField(\"PYOtherExpensesAmt\", LongType(), True),\n",
        "    StructField(\"CYOtherExpensesAmt\", LongType(), True),\n",
        "    StructField(\"PYTotalExpensesAmt\", LongType(), True),\n",
        "    StructField(\"CYTotalExpensesAmt\", LongType(), True),\n",
        "    StructField(\"PYRevenuesLessExpensesAmt\", LongType(), True),\n",
        "    StructField(\"CYRevenuesLessExpensesAmt\", LongType(), True),\n",
        "    StructField(\"TotalAssetsBOYAmt\", LongType(), True),\n",
        "    StructField(\"TotalAssetsEOYAmt\", LongType(), True),\n",
        "    StructField(\"TotalLiabilitiesBOYAmt\", LongType(), True),\n",
        "    StructField(\"TotalLiabilitiesEOYAmt\", LongType(), True),\n",
        "    StructField(\"NetAssetsOrFundBalancesBOYAmt\", LongType(), True),\n",
        "    StructField(\"NetAssetsOrFundBalancesEOYAmt\", LongType(), True),\n",
        "    StructField(\"ActivityOrMissionDesc\", StringType(), True)\n",
        "])\n",
        "\n",
        "e990_df = spark.read.format(\"csv\") \\\n",
        "    .option(\"delimiter\", \"|\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(schema) \\\n",
        "    .load(f\"{folder_in}/eo990extract.csv\")\n",
        "\n",
        "spark.sql(\"DROP TABLE IF EXISTS e990\") # make repeatable\n",
        "e990_df.write.saveAsTable('e990')\n",
        "# end"
      ],
      "metadata": {
        "id": "vnyvwloaP1I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EO 990 Prepare EOMF"
      ],
      "metadata": {
        "id": "GGjaY1OJYVxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EO 990 Prepare EOMF\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "appName = \"eo990-prepare-eomf\"\n",
        "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
        "if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
        "    folder_in = \"/mnt/eo990pipeline\"\n",
        "else: # local spark - no cluster charges!!\n",
        "    folder_in = \".\"\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
        "schema = StructType([\n",
        "    StructField(\"EIN\", StringType(), True),\n",
        "    StructField(\"NAME\", StringType(), True),\n",
        "    StructField(\"ICO\", StringType(), True),\n",
        "    StructField(\"STREET\", StringType(), True),\n",
        "    StructField(\"CITY\", StringType(), True),\n",
        "    StructField(\"STATE\", StringType(), True),\n",
        "    StructField(\"ZIP\", StringType(), True),\n",
        "    StructField(\"RULING\", StringType(), True),\n",
        "    StructField(\"TAX_PERIOD\", StringType(), True),\n",
        "    StructField(\"GROUP\", StringType(), True),\n",
        "    StructField(\"SUBSECTION\", StringType(), True),\n",
        "    StructField(\"AFFILIATION\", StringType(), True),\n",
        "    StructField(\"CLASSIFICATION\", StringType(), True),\n",
        "    StructField(\"DEDUCTIBILITY\", StringType(), True),\n",
        "    StructField(\"FOUNDATION\", StringType(), True),\n",
        "    StructField(\"ACTIVITY\", StringType(), True),\n",
        "    StructField(\"ORGANIZATION\", StringType(), True),\n",
        "    StructField(\"STATUS\", StringType(), True),\n",
        "    StructField(\"ASSET_CD\", StringType(), True),\n",
        "    StructField(\"INCOME_CD\", StringType(), True),\n",
        "    StructField(\"FILING_REQ_CD\", StringType(), True),\n",
        "    StructField(\"PF_FILING_REQ_CD\", StringType(), True),\n",
        "    StructField(\"ACCT_PD\", StringType(), True),\n",
        "    StructField(\"ASSET_AMT\", LongType(), True),\n",
        "    StructField(\"INCOME_AMT\", LongType(), True),\n",
        "    StructField(\"REVENUE_AMT\", LongType(), True),\n",
        "    StructField(\"NTEE_CD\", StringType(), True),\n",
        "    StructField(\"SORT_NAME\", StringType(), True)\n",
        "])\n",
        "\n",
        "eomf_df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(schema) \\\n",
        "    .load(f\"{folder_in}/eo[1234].csv\") # Concatenate input files - yes!\n",
        "\n",
        "from pyspark.sql.functions import col, substring\n",
        "eomf_df = eomf_df.fillna({'NTEE_CD': 'Z99'})\n",
        "eomf_df = eomf_df.withColumn(\"NTEE3\", substring(col(\"NTEE_CD\"), 1, 3))\n",
        "eomf_df = eomf_df.withColumnRenamed(\"GROUP\", \"GROUP_NUM\") # GROUP is reserved\n",
        "\n",
        "spark.sql(\"DROP TABLE IF EXISTS eomf\")\n",
        "eomf_df.write.saveAsTable('eomf')\n",
        "# end"
      ],
      "metadata": {
        "id": "LlE2EvzbYWLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EO 990 Prepare GCST"
      ],
      "metadata": {
        "id": "zJcl5YqKogKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EO 990 Prepare GCST\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "appName = \"eo990-prepare-gcst\"\n",
        "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
        "if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
        "    folder_in = \"/mnt/eo990pipeline\"\n",
        "else: # local spark - no cluster charges!!\n",
        "    folder_in = \".\"\n",
        "\n",
        "from pyspark.sql.types import \\\n",
        "    StructType, StructField, StringType, IntegerType, LongType\n",
        "schema = StructType([\n",
        "    StructField(\"code\", StringType(), True),\n",
        "    StructField(\"desc\", StringType(), True),\n",
        "])\n",
        "gcst_df = spark.read.format(\"csv\") \\\n",
        "    .option(\"delimiter\", \"|\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(schema) \\\n",
        "    .load(f\"{folder_in}/gcst.csv\")\n",
        "gcst_df = gcst_df.withColumnRenamed(\"desc\", \"SECTOR\") # desc is reserved in SQL\n",
        "\n",
        "spark.sql(\"DROP TABLE IF EXISTS gcst\")\n",
        "gcst_df.write.saveAsTable('gcst')"
      ],
      "metadata": {
        "id": "QlQVVzRkofQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EO 990 Transform"
      ],
      "metadata": {
        "id": "R-bN2b9Pqu1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EO 990 Transform\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"eo990-transform\").getOrCreate()\n",
        "eo990_df = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    eomf.EIN, eomf.NAME, eomf.ICO, eomf.STREET, eomf.CITY, eomf.STATE, eomf.ZIP,\n",
        "    eomf.RULING, eomf.TAX_PERIOD, eomf.GROUP_NUM, eomf.SUBSECTION,\n",
        "    eomf.AFFILIATION, eomf.CLASSIFICATION, eomf.DEDUCTIBILITY, eomf.FOUNDATION,\n",
        "    eomf.ACTIVITY, eomf.ORGANIZATION, eomf.STATUS, eomf.ASSET_CD,\n",
        "    eomf.INCOME_CD, eomf.FILING_REQ_CD, eomf.PF_FILING_REQ_CD, eomf.ACCT_PD,\n",
        "    eomf.ASSET_AMT, eomf.INCOME_AMT, eomf.REVENUE_AMT, eomf.NTEE_CD,\n",
        "    eomf.SORT_NAME, eomf.NTEE3, gcst.SECTOR,\n",
        "    e990.BLOBI, e990.ReturnTypeCd, e990.TaxPeriodEndDt, e990.BusinessName,\n",
        "    e990.BusinessStreet, e990.CityNm, e990.StateAbbreviationCd, e990.ZIPCd,\n",
        "    e990.WebsiteAddressTxt, e990.TotalEmployeeCnt, e990.TotalVolunteersCnt,\n",
        "    e990.GrossReceiptsAmt,\n",
        "    e990.PYContributionsGrantsAmt, e990.CYContributionsGrantsAmt,\n",
        "    e990.PYProgramServiceRevenueAmt, e990.CYProgramServiceRevenueAmt,\n",
        "    e990.PYInvestmentIncomeAmt, e990.CYInvestmentIncomeAmt,\n",
        "    e990.PYOtherRevenueAmt, e990.CYOtherRevenueAmt,\n",
        "    e990.PYTotalRevenueAmt, e990.CYTotalRevenueAmt,\n",
        "    e990.PYGrantsAndSimilarPaidAmt, e990.CYGrantsAndSimilarPaidAmt,\n",
        "    e990.PYBenefitsPaidToMembersAmt, e990.CYBenefitsPaidToMembersAmt,\n",
        "    e990.PYSalariesCompEmpBnftPaidAmt, e990.CYSalariesCompEmpBnftPaidAmt,\n",
        "    e990.PYTotalProfFndrsngExpnsAmt, e990.CYTotalProfFndrsngExpnsAmt,\n",
        "    e990.CYTotalFundraisingExpenseAmt,\n",
        "    e990.PYOtherExpensesAmt, e990.CYOtherExpensesAmt,\n",
        "    e990.PYTotalExpensesAmt, e990.CYTotalExpensesAmt,\n",
        "    e990.PYRevenuesLessExpensesAmt, e990.CYRevenuesLessExpensesAmt,\n",
        "    e990.TotalAssetsBOYAmt, e990.TotalAssetsEOYAmt,\n",
        "    e990.TotalLiabilitiesBOYAmt, e990.TotalLiabilitiesEOYAmt,\n",
        "    e990.NetAssetsOrFundBalancesBOYAmt, e990.NetAssetsOrFundBalancesEOYAmt,\n",
        "    e990.ActivityOrMissionDesc\n",
        "FROM eomf\n",
        "INNER JOIN e990 ON eomf.EIN = e990.EIN\n",
        "LEFT JOIN gcst ON eomf.NTEE3 = gcst.code\n",
        "\"\"\")\n",
        "spark.sql(\"DROP TABLE IF EXISTS eo990\")\n",
        "eo990_df.write.saveAsTable('eo990')"
      ],
      "metadata": {
        "id": "fiLbZyGgqP0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EO 990 Learn"
      ],
      "metadata": {
        "id": "D8jikI3cynku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EO 990 Learn\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import log, col\n",
        "from pyspark.sql.functions import col, lit\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Launch the awesome Spark Engine!!\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"eo990-cluster-simple\") \\\n",
        "    .config(\"spark.databricks.photon.enabled\", \"false\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# The objective is a Kmeans clustering of public US tax exempt organizations\n",
        "# based the employee count, volunteer count, and contribution/grant amount\n",
        "# recorded in their most recent 990 filing. We start by selecting the data:\n",
        "\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT eo990.EIN, eo990.CYContributionsGrantsAmt,\n",
        "           eo990.TotalEmployeeCnt, eo990.TotalVolunteersCnt\n",
        "    FROM eo990\n",
        "    WHERE TotalVolunteersCnt < 1000000\n",
        "    AND TotalEmployeeCnt > 0\n",
        "    AND TotalVolunteersCnt > 0\n",
        "    AND CYContributionsGrantsAmt > 0\n",
        "\"\"\")\n",
        "\n",
        "# All of these features exhibit a long-tailed distribution of values; the\n",
        "# larger the values, the less frequently they occur. As with most machine\n",
        "# learning tasks, a more uniform distribution is needed for the algorithm\n",
        "# to function effectively. A log transformation is applied to acheive this\n",
        "\n",
        "df_transformed = df.withColumn(\n",
        "    \"log_CYContributionsGrantsAmt\", log(col(\"CYContributionsGrantsAmt\") + 1))\n",
        "df_transformed = df_transformed.withColumn(\n",
        "    \"log_TotalEmployeeCnt\", log(col(\"TotalEmployeeCnt\") + 1))\n",
        "df_transformed = df_transformed.withColumn(\n",
        "    \"log_TotalVolunteersCnt\", log(col(\"TotalVolunteersCnt\") + 1))\n",
        "\n",
        "# Null values and outliers are removed:\n",
        "\n",
        "df_transformed = df_transformed.dropna()\n",
        "bounds = {\n",
        "    c: dict(\n",
        "        zip([\"q1\", \"q3\"], df_transformed.approxQuantile(c, [0.25, 0.75], 0))\n",
        "    )\n",
        "    for c in [\"log_CYContributionsGrantsAmt\",\n",
        "              \"log_TotalEmployeeCnt\", \"log_TotalVolunteersCnt\"]\n",
        "}\n",
        "for c in bounds:\n",
        "    iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
        "    bounds[c]['lower_bound'] = bounds[c]['q1'] - (1.5 * iqr)\n",
        "    bounds[c]['upper_bound'] = bounds[c]['q3'] + (1.5 * iqr)\n",
        "for c in bounds:\n",
        "    df_transformed = df_transformed.filter(\n",
        "        (col(c) >= bounds[c]['lower_bound']) &\n",
        "        (col(c) <= bounds[c]['upper_bound'])\n",
        "    )\n",
        "\n",
        "# A pipeline assembling the features into a vector is contructed with\n",
        "# a standard scaler.\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"log_CYContributionsGrantsAmt\",\n",
        "               \"log_TotalEmployeeCnt\", \"log_TotalVolunteersCnt\"],\n",
        "    outputCol=\"features\")\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "pipeline = Pipeline(stages=[assembler, scaler])\n",
        "pipelineModel = pipeline.fit(df_transformed)\n",
        "df_kmeans_ready = pipelineModel.transform(df_transformed)\n",
        "\n",
        "# A KMeans function is setup and invoked, and the counts for each K value\n",
        "# are displayed\n",
        "\n",
        "def apply_kmeans(df, k):\n",
        "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"scaledFeatures\")\n",
        "    model = kmeans.fit(df)\n",
        "    return model.transform(df)\n",
        "sample = df_kmeans_ready.sample(withReplacement=False, fraction=1.0, seed=1)\n",
        "eo990clustering_df = apply_kmeans(sample, 4)\n",
        "\n",
        "spark.sql(\"DROP TABLE IF EXISTS eo990clustering\")\n",
        "eo990clustering_df.write.saveAsTable('eo990clustering')"
      ],
      "metadata": {
        "id": "1pKmkTP9tNfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EO 990 Hydrate Clustering"
      ],
      "metadata": {
        "id": "mdfI9GSmSZJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "import plotly.express as px\n",
        "\n",
        "appName = \"eo990-hydrate\"\n",
        "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
        "\n",
        "if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
        "    dbfs = \"/dbfs\"\n",
        "    folder_out = \"/mnt/eo990pipeline\"\n",
        "else: # local setup - no cluster charges!!\n",
        "    dbfs = \"\"\n",
        "    folder_out = \".\"\n",
        "\n",
        "hydrate_df = spark.sql(\"\"\"\n",
        "    SELECT CYContributionsGrantsAmt AS ContributionsGrants,\n",
        "         TotalEmployeeCnt AS Employees,\n",
        "         TotalVolunteersCnt AS Volunteers,\n",
        "         CAST(prediction AS STRING) AS Cluster\n",
        "    FROM eo990clustering\n",
        "\"\"\")\n",
        "subset_df = hydrate_df.sample(\n",
        "    withReplacement=False, fraction=0.5, seed=1)\n",
        "pdf = subset_df.toPandas()\n",
        "color_map = {\n",
        "    '0': '#1ac958',  # green\n",
        "    '1': '#db5d51',  # red\n",
        "    '2': '#fac828',  # gold\n",
        "    '3': '#5385f4',  # blue\n",
        "}\n",
        "fig = px.scatter_3d(\n",
        "    pdf,\n",
        "    x='ContributionsGrants',\n",
        "    y='Employees',\n",
        "    z='Volunteers',\n",
        "    color='Cluster',color_discrete_map=color_map,\n",
        "    title='Exempt Organization Kmeans Clustering K=4')\n",
        "fig.write_html(f'{dbfs}{folder_out}/clustering.html', include_plotlyjs='cdn')\n",
        "\n",
        "# and that ends the pipeline!\n",
        "\n",
        "fig.show() # can be commented out in production"
      ],
      "metadata": {
        "id": "yWeVaSHBvfoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EO 990 Hydrate Parallel Charting"
      ],
      "metadata": {
        "id": "IJSakGAKgK2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Charting Functions"
      ],
      "metadata": {
        "id": "emd5jiAmGbol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "#from IPython.core.display import HTML\n",
        "\n",
        "def treemap_replaces(s): # restyle plotly output\n",
        "    s = s.replace('<body>', '<body style=\"margin:0; overflow=\"hidden\">')\n",
        "    s = s.replace('</head>',\n",
        "        '<style>.svg-container { overflow: hidden; }</style></head>')\n",
        "    return s\n",
        "\n",
        "def giving_treemap_nonprofits(df): # called by parallel process\n",
        "\n",
        "    ntee3 = df['NTEE3'].iloc[0]\n",
        "    sector_desc = df['SECTOR'].iloc[0]\n",
        "\n",
        "    # create groupings by contribution amount\n",
        "    cy_amt_col = 'CYContributionsGrantsAmt'\n",
        "    df[\"level\"] = pd.cut(df[cy_amt_col],\n",
        "        [-np.inf, 999_999.999, 2_499_999.999, 9_999_999.999,\n",
        "               49_999_999.999, np.inf],\n",
        "        labels=[\n",
        "            '<b>Under $1M</b>', '<b>$1M-<$2.5M</b>', '<b>$2.5M-<$10M</b>',\n",
        "            '<b>$10M-<$50M</b>','<b>$50M and Up</b>']\n",
        "    )\n",
        "\n",
        "    # limit to the largest organizations\n",
        "    threshold = 800\n",
        "    if df.shape[0] < threshold:\n",
        "        cutoff = 0\n",
        "    elif df[df[cy_amt_col] >=  1_000_000].shape[0] < threshold:\n",
        "        cutoff =  1_000_000\n",
        "    elif df[df[cy_amt_col] >=  2_500_000].shape[0] < threshold:\n",
        "        cutoff =  2_500_000\n",
        "    elif df[df[cy_amt_col] >= 10_000_000].shape[0] < threshold:\n",
        "        cutoff = 10_000_000\n",
        "    elif df[df[cy_amt_col] >= 50_000_000].shape[0] < threshold:\n",
        "        cutoff = 50_000_000\n",
        "    else:\n",
        "        cutoff = df.at[threshold-1,cy_amt_col]\n",
        "    df[\"nonprofit\"] = np.where(df[cy_amt_col] >= cutoff,df[\"EIN\"], \"Total\")\n",
        "\n",
        "    # regroup to collapse smaller organizations into one box\n",
        "    adf2 = df.groupby(['level','nonprofit']).agg(\n",
        "        py_amount=('PYContributionsGrantsAmt', 'sum'),\n",
        "        cy_amount=('CYContributionsGrantsAmt', 'sum'),\n",
        "        count=('EIN', 'count'),\n",
        "    )\n",
        "\n",
        "    # calculate contribution growth\n",
        "    adf2 = adf2[adf2[\"cy_amount\"] > 0].copy()\n",
        "    adf2['py_amount'] = adf2['py_amount'].round()\n",
        "    adf2['cy_amount'] = adf2['cy_amount'].round()\n",
        "    adf2[\"growth\"] = np.where(adf2[\"py_amount\"] == 0,0,\n",
        "        ( adf2[\"cy_amount\"] - adf2[\"py_amount\"]) * 100.0\n",
        "        / adf2[\"py_amount\"])\n",
        "\n",
        "    # cap growth to 100% for color balance\n",
        "    adf2[\"growth\"] = np.where(\n",
        "        adf2[\"growth\"] > 100, 100, adf2[\"growth\"]).astype(float)\n",
        "\n",
        "    adf2 = adf2.reset_index()\n",
        "\n",
        "    adf2 = adf2.merge(\n",
        "        df[['EIN','BusinessName', 'CITY', 'STATE']],\n",
        "        left_on='nonprofit', right_on='EIN', how='left')\n",
        "    adf2['BusinessName'].fillna('', inplace=True)\n",
        "    adf2['CITY'].fillna('', inplace=True)\n",
        "    adf2['STATE'].fillna('', inplace=True)\n",
        "    adf2['EIN'].fillna('Total', inplace=True)\n",
        "    adf2[\"link\"] = np.where(adf2[\"nonprofit\"] == \"Total\",\"\",\"🔗\")\n",
        "\n",
        "    # chart it in plotly!\n",
        "    sector_desc = f'{sector_desc} ({ntee3})'\n",
        "    fig = px.treemap(\n",
        "        adf2,\n",
        "        path=[px.Constant(sector_desc), 'level', 'EIN'],\n",
        "        values='cy_amount',\n",
        "        color='growth',\n",
        "        color_continuous_scale=['red','white','green'],\n",
        "        range_color=[-100, 100],\n",
        "        color_continuous_midpoint=0,\n",
        "        custom_data=['BusinessName', 'CITY', 'STATE', 'link']\n",
        "    )\n",
        "\n",
        "    fig.update_layout(margin = dict(t=0, l=0, r=0, b=0))\n",
        "    fig.update_traces(marker_line_width = 0, tiling_pad = 1)\n",
        "    labels = list(fig.data[0].labels)\n",
        "\n",
        "    fig.data[0].hovertemplate = \"\"\"\\\n",
        "%{customdata[0]}<br>$%{value:,.0f}}\n",
        "\"\"\"\n",
        "    fig.data[0].texttemplate = \"\"\"\\\n",
        "%{customdata[0]}<br>$%{value:,.0f}<br><a\n",
        "href=\"https://projects.propublica.org/nonprofits/organizations/%{label}\"\n",
        "style=\"cursor: help; color: blue;\" rel=\"noopener noreferrer\"\n",
        ">%{customdata[3]}</a>\"\"\"\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "yUtOYW8FkOSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serial Testing"
      ],
      "metadata": {
        "id": "rbdHYZU6HVqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "appName = \"eo990-hydrate-parallel-charting-testing\"\n",
        "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
        "\n",
        "pandas_df = spark.sql(\"\"\"\n",
        "    SELECT EIN, NTEE3, ntee1.SECTOR AS SECTOR1, eo990.SECTOR,\n",
        "        ReturnTypeCd, TaxPeriodEndDt, BusinessName, CITY, STATE,\n",
        "        PYContributionsGrantsAmt, CYContributionsGrantsAmt,\n",
        "        PYGrantsAndSimilarPaidAmt, CYGrantsAndSimilarPaidAmt,\n",
        "        PYTotalProfFndrsngExpnsAmt, CYTotalProfFndrsngExpnsAmt,\n",
        "        CYTotalFundraisingExpenseAmt,\n",
        "        TotalEmployeeCnt, TotalVolunteersCnt\n",
        "    FROM eo990\n",
        "    JOIN gcst AS ntee1 ON LEFT(eo990.NTEE3,1) = ntee1.code\n",
        "    JOIN gcst AS ntee3 ON LEFT(eo990.NTEE3,3) = ntee3.code\n",
        "    WHERE CYContributionsGrantsAmt > 0\n",
        "    AND NTEE3 = 'K31' -- just one for testing\n",
        "\"\"\").toPandas()\n",
        "\n",
        "giving_treemap_nonprofits(pandas_df).show()"
      ],
      "metadata": {
        "id": "SWTtbd_PHm1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel Processing"
      ],
      "metadata": {
        "id": "Tx5_bX7zGrhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
        "    dbfs = \"/dbfs\"\n",
        "    folder_out = \"/mnt/eo990pipeline\"\n",
        "else: # local setup - no cluster charges!!\n",
        "    dbfs = \"\"\n",
        "    folder_out = \".\"\n",
        "\n",
        "if os.path.isdir(f\"{dbfs}{folder_out}/draft\"):\n",
        "    shutil.rmtree(f\"{dbfs}{folder_out}/draft\")\n",
        "os.makedirs(f\"{dbfs}{folder_out}/draft\")\n",
        "\n",
        "appName = \"eo990-hydrate-parallel-charting\"\n",
        "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
        "\n",
        "chart_df = spark.sql(\"\"\"\n",
        "    SELECT EIN, NTEE3, ntee1.SECTOR AS SECTOR1, eo990.SECTOR,\n",
        "        ReturnTypeCd, TaxPeriodEndDt, BusinessName, CITY, STATE,\n",
        "        PYContributionsGrantsAmt, CYContributionsGrantsAmt,\n",
        "        PYGrantsAndSimilarPaidAmt, CYGrantsAndSimilarPaidAmt,\n",
        "        PYTotalProfFndrsngExpnsAmt, CYTotalProfFndrsngExpnsAmt,\n",
        "        CYTotalFundraisingExpenseAmt,\n",
        "        TotalEmployeeCnt, TotalVolunteersCnt\n",
        "    FROM eo990\n",
        "    JOIN gcst AS ntee1 ON LEFT(eo990.NTEE3,1) = ntee1.code\n",
        "    JOIN gcst AS ntee3 ON LEFT(eo990.NTEE3,3) = ntee3.code\n",
        "    WHERE CYContributionsGrantsAmt > 0\n",
        "    --AND NTEE3 IN ('K31','B43')\n",
        "\"\"\")\n",
        "\n",
        "df = chart_df.repartition('NTEE3')\n",
        "rdd = chart_df.rdd.keyBy(lambda row: row.NTEE3)\n",
        "grouped_rdd = rdd.groupByKey()\n",
        "print(\"Number of NTEE3: \", grouped_rdd.count())\n",
        "#print(\"List of NTEE3s: \", sorted(grouped_rdd.keys().collect()))\n",
        "COLUMNS = spark.sparkContext.broadcast(chart_df.columns)\n",
        "\n",
        "def process_group(iterator):\n",
        "    pandas_df = pd.DataFrame(list(iterator), columns=COLUMNS.value)\n",
        "    ntee3 = pandas_df['NTEE3'].iloc[0]\n",
        "\n",
        "    fig = giving_treemap_nonprofits(pandas_df) # create chart figure\n",
        "    path = f'{dbfs}{folder_out}/draft/sector_treemap_{ntee3}.html'\n",
        "    with open(path, 'w', encoding='utf_8') as f:\n",
        "        #f.write(\"testing\")\n",
        "        src = fig.to_html(full_html=True, include_plotlyjs='cdn')\n",
        "        f.write(treemap_replaces(src))\n",
        "\n",
        "    return path\n",
        "\n",
        "result_rdd = grouped_rdd.mapValues(process_group)\n",
        "collected_results = result_rdd.collect()\n",
        "#for ntee3, path in collected_results:\n",
        "#    print(f\"{ntee3}: {path}\")"
      ],
      "metadata": {
        "id": "7Z__9FMsHGq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bottom"
      ],
      "metadata": {
        "id": "wOIQ-L-Fgpb4"
      }
    }
  ]
}